# Журнал выполнения проекта "gb_altinvest"
(Диплом по направлению "Специалист по ИИ", 2025)

---

## 2025-05-21

### 1. Сформулирован план реализации дипломного проекта
Определена основная идея работы: анализ взаимосвязи альтернативных данных (вакансии, новости) с котировками акций. Зафиксированы цели, задачи и ожидаемые результаты.

### 2. Выбран технологический стек
Принят следующий стек технологий:
- Python (pandas, plotly, sklearn) — для анализа и визуализации данных
- PHP — для реализации веб-интерфейса
- Docker (docker-compose) — для изоляции и автоматизации развёртывания
- Airflow — для организации ETL и расписания
- (Опционально) Telegram-бот для доставки результатов

### 3. Попытка получения архивных котировок по Казахстану (KASE, AIX)
Осуществлён поиск и предпринята попытка получения архивных котировок по тикерам Kcell (KCEL), Halyk Bank (HSBK), КазАтомПром (KAP), а также по индексу KASE с официальных источников KASE и AIX.  
Установлено, что архивы по тикерам доступны через API (например, Tradernet), а архив индекса KASE в пригодном виде отсутствует. Автоматизация получения данных по индексу KASE признана затруднительной.

### 4. Корректировка набора тикеров
В связи с отсутствием индекса KASE в удобном для работы формате, принято решение исключить его из анализа. Для дальнейшей работы утверждён следующий набор тикеров: Kcell (KCEL), Halyk Bank (HSBK), КазАтомПром (KAP/KZAP/KAP.AIX).

### 5. Принято решение о резервном плане на случай проблем с данными
В случае невозможности получения достаточного объёма данных по казахстанским бумагам подготовлен альтернативный сценарий перехода на российские инструменты с выгрузкой архивов с moex.com.

### 6. Создан репозиторий и инициализирован проект
Создан репозиторий на GitHub (`gb_altinvest`), добавлен файл README.md, настроена синхронизация с локальной папкой GB_AI_Specialist_Diplom. Зафиксирована структура основных рабочих папок.

---

## 2025-05-22

### 7. Принято решение реализовать автоматизированный сбор исторических данных через интеграцию с Tradernet API
Проведено тестирование работы с Tradernet Public API. Получение исторических котировок по необходимым тикерам прошло успешно. Принято решение реализовать автоматизированный сбор и обновление данных с использованием данного API в рамках собственного веб-сервиса.

### 8. Начата разработка контейнеризованного веб-сервиса
Принято решение о создании микросервисной архитектуры проекта с использованием контейнеризации (Docker Compose). Начата настройка docker-compose.yml с сервисами PHP (web), MySQL (db), phpMyAdmin, python-analysis, airflow, telegram-bot.

### 9. Добавлены и описаны файлы конфигурации php.ini и Apache (vhosts/default.conf)
Внесены рабочие настройки лимитов, часового пояса, вывода ошибок в php.ini. Сконфигурирован виртуальный хост Apache с разрешением .htaccess и настройками для разработки и тестирования. Описание и рекомендации вынесены в README.md.

### 10. Разработан и внедрён .htaccess для html/ и siteroot/
Созданы файлы .htaccess для корневой папки html и директории siteroot/ с правилами защиты, маршрутизации запросов, доступа к статикам и ограничения доступа к системным папкам. Описание внесено в README.md.

### 11. Создан файл index.php для siteroot/
Разработан минималистичный файл index.php, реализующий базовую логику инициализации, обработки ошибок и запуска основного приложения. В файле обеспечено подключение конфигурации, регистрация обработчика ошибок и запуск приложения через $app->run().

### 12. Создан и внедрён контейнер python-analysis с интеграцией веб-сервиса для сбора данных
Создан отдельный контейнер python-analysis для автоматизации сбора и анализа данных. Реализована интеграция с внутренним PHP-веб-сервисом, через который осуществляется доступ к Tradernet API (подготовлен отдельный класс-клиент, проведено тестирование).  
Настроено монтирование volume ./data/python/ для обмена данными и результатами между сервисами.

### 13. Выполнена сборка и запуск всех сервисов через Docker Compose
Проект собран и запущен с помощью docker-compose up --build. Проверена работоспособность всех ключевых сервисов, структура volumes и конфигов.

---

# (Все рутинные настройки volumes, служебные папки и .gitkeep фиксируются только в README.md, а не в history.md)

## 2025-05-22

### 19. Создан и структурирован файл config/init.json для приложения AltInSight
В папке config создан и оформлен файл init.json, в котором централизованно хранятся настройки приложения, параметры авторизации и безопасности, события для логирования, подключение к базе данных и ключи интеграции с Tradernet. Описание структуры файла добавлено в README.md.

### 20. Создан базовый web-контроллер для тестирования интеграции с Tradernet API
Реализован метод testTradernetApi, позволяющий получать и отображать котировки по тикеру KCEL через веб-интерфейс.  
Контроллер подготовлен к расширению для работы с другими тикерами и доработке шаблонов вывода данных. Планируется добавить обработку ошибок, универсализацию тикеров и улучшить автозагрузку классов.

### 21. Актуализирована и зафиксирована структура проекта после реализации веб-сервиса
Структура проекта расширена и уточнена: 
- В папке html/ реализована чёткая структура MVC: app/Core, app/Domain, app/Infrastructure, app/Presentation;
- Добавлены контроллеры и хендлеры для API (html/API/);
- Все рабочие и обменные папки (data/python, data/tgbot, logs/, storage/logs/) оформлены и снабжены служебными файлами .gitkeep;
- Описаны новые уровни абстракции для ядра, сервисов и клиента внешних API.
Актуальная структура проекта добавлена в README.md и используется для сопровождения дальнейшей документации.

### 22. Реализован сервис экспорта котировок в JSON/CSV для Python-аналитики
Создан отдельный QuotesExportService, обеспечивающий сохранение котировок по каждому тикеру в формате JSON и CSV в общую папку data/python/quotes/ (volume, доступный для PHP и Python-контейнеров).

Все экспортируемые файлы имеют актуальное содержимое, старые версии не накапливаются — всегда сохраняется свежий срез по каждому тикеру.

### 23. Разработан CLI-скрипт для пакетного экспорта котировок
Написан отдельный PHP-скрипт (export_all_quotes.php), обеспечивающий автоматический экспорт котировок для всех поддерживаемых тикеров за указанный период.

Скрипт может запускаться вручную, через cron или из DAG Airflow, поддерживает централизованную интеграцию между сервисами и аналитическим пайплайном.

Исправлены ошибки путей и организации volume в docker-compose; скрипт корректно подключает ядро приложения из /var/www/html/bootstrap.php.

### 24. Организован централизованный volume обмена данными между сервисами
В docker-compose.yml настроено одновременное монтирование data/python для сервисов web (PHP), python-analysis и airflow.

Все сервисы могут читать и писать общие данные (quotes) для передачи между этапами ETL и аналитики.

### 25. Проведено тестирование обмена: полная выгрузка котировок из Tradernet, запись в файл, последующее чтение из Python
Проверена работоспособность сценария: экспорт → чтение в pandas.

Все обменные форматы и скрипты готовы к автоматизации через Airflow DAG.

### 26. Интеграция Apache Airflow и автоматизация экспорта данных
- В проект добавлен контейнер с Apache Airflow для автоматизации ETL-операций и организации расписания регулярного экспорта котировок.
- В папке dags/ создан DAG, реализующий регулярный запуск PHP-скрипта для экспорта котировок по всем тикерам в формате JSON (BashOperator → php /data/python/export_all_quotes.php).
- Проведена настройка прав доступа и разруливание ошибок входа в web-интерфейс Airflow (reset базы, пересоздание пользователя admin, удаление “битых” users, сброс пароля через CLI).
- Организовано монтирование volumes для обмена данными между Airflow, PHP и Python-аналитикой.
- Протестирована работа пайплайна: данные, экспортированные из Tradernet, автоматически доступны для последующего анализа в Python через DAG Airflow.
- Инструкции по работе с Airflow и типовые команды добавлены в README.md и внутреннюю документацию.

### 27. Сформулирована задача сбора альтернативных данных
- Определена архитектура и порядок интеграции альтернативных данных (вакансии, новости, события) с основным аналитическим сервисом.
- Для каждого типа альтернативных данных предусмотрено:
    - Выбор одного или нескольких надёжных и легальных источников информации (API, open-data, RSS-ленты);
    - Реализация отдельного клиента (сервиса) для автоматизированного или полуавтоматического сбора данных;
    - Единый формат хранения (CSV/JSON) в общей папке для обмена между сервисами и аналитикой;
    - Возможность запуска сбора данных по расписанию (Airflow DAG или вручную через отдельный скрипт).
- Намечена реализация контроллера и веб-интерфейса для отображения и базовой работы с собранными альтернативными данными (например, страница по вакансиям и новостям для выбранной компании).
- Система сбора альтернативных данных проектируется с возможностью дальнейшего расширения перечня источников по мере роста и развития сервиса.

### 28. Принятие решения по интеграции контейнеров PHP и Python
- Проведён сравнительный анализ трёх вариантов интеграции между контейнерами (вызов скрипта через DockerOperator, установка PHP в Airflow, REST API взаимодействие).
- Принято решение реализовать интеграцию через REST API:
    - PHP-контейнер предоставляет endpoint для запуска экспортных и аналитических скриптов, а также отдачи данных;
    - Python-аналитика (python-analysis) обращается к этим endpoint-ам через HTTP-запросы, получает свежие данные и сохраняет результаты обратно (через REST или в общую файловую папку);
    - Такой подход обеспечивает универсальность, масштабируемость и прозрачность обмена между сервисами.
- Airflow отвечает только за планирование и оркестрацию задач; анализ и интеграция выполняются в контейнере python-analysis.

### 29. Принятие регламента размещения скриптов и правил обмена данными между сервисами
- Все Python-скрипты для аналитики и обмена данными размещаются в директории /data/python/scripts/, которая проброшена через volume в оба контейнера — python-analysis и (при необходимости) airflow.
- Для взаимодействия между сервисами (PHP и Python) используется единая папка обмена данными — /data/python/:
    - PHP-скрипты сохраняют выгрузки данных (котировки, вакансии и пр.) в формате JSON в /data/python/quotes/, /data/python/vacancies/ и т.д.
    - Python-скрипты читают эти файлы, проводят анализ и сохраняют результаты обработки (например, отчёты или визуализации) в /data/python/results/.
- Такой подход обеспечивает прозрачный и надёжный обмен данными между всеми компонентами системы, а также облегчает организацию автоматических ETL-процессов через Airflow и доступ к результатам анализа из web-интерфейса PHP.

### 22. Исправлены механизмы определения и обработки API-запросов
Проблема:
При вызове API-эндпоинта возвращался HTML, а не JSON, из-за некорректной работы метода определения “API-запроса” (isApiStatic()), что мешало Python-скриптам парсить результат.

Действия:
- Проанализирована логика работы роутинга и ErrorHandler-а.
- Метод определения API-запросов (isApiStatic()) доработан: теперь учитывается любой вариант пути (/api/, /API/, чувствительность к регистру устранена).
- Для всех запросов в папку /API теперь всегда возвращается JSON, вне зависимости от возникновения ошибок.
- Обработчик ошибок для API вынесен в отдельный блок: uncaught exception теперь всегда возвращает корректный JSON-ответ, а не HTML-шаблон.
- Проведено тестирование как из браузера, так и из Python-кода и через Airflow: все запросы к API отдают корректный JSON.

Результат:
API-запросы теперь всегда возвращают JSON, в том числе в случае ошибок.
Проблема с парсингом в Python-скрипте и в DAG Airflow полностью устранена.
Механизм взаимодействия между PHP-backend и Python теперь устойчив и стандартизирован.

